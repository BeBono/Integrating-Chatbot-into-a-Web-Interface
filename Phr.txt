

*****Create a docker file manually / Python version 3.10:

# Use the official Python image as a base
FROM python:3.10

# Set the working directory
WORKDIR /app

# Copy the requirements file into the image
COPY requirements.txt .

# Install the necessary Python packages
RUN pip install --no-cache-dir -r requirements.txt

# Copy the rest of your application code
COPY . .

# Define the command to run your application
CMD ["python", "your_script.py"]


**********************

Deploy your application en IBM Cloud Code Engine:

ibmcloud ce project get --name "Code Engine - sn-labs-albertocarb1"


ibmcloud ce build create --name build-local-dockerfile2 \
                        --build-type local --size small \
                        --image us.icr.io/${SN_ICR_NAMESPACE}/myapp2 \
                        --registry-secret icr-secret
                        /


ibmcloud ce buildrun submit --name buildrun-local-dockerfile2 \
                            --build build-local-dockerfile2 \
                            --source .
                            /

ibmcloud ce buildrun get -n buildrun-local-dockerfile2


******Module 2: Create Your Own ChatGPT-like Website

https://author-ide.skills.network/render?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJtZF9pbnN0cnVjdGlvbnNfdXJsIjoiaHR0cHM6Ly9jZi1jb3Vyc2VzLWRhdGEuczMudXMuY2xvdWQtb2JqZWN0LXN0b3JhZ2UuYXBwZG9tYWluLmNsb3VkL0lCTVNraWxsc05ldHdvcmstQUkwMzMxRU4tU2tpbGxzTmV0d29yay9sYWJzJTJGQ2xvdWRJREVfLV9DcmVhdGVfQ2hhdEdQVC1saWtlX1dlYnNpdGVfd2l0aF9PcGVuX1NvdXJjZV9MTE1zLm1kIiwidG9vbF90eXBlIjoidGhlaWEiLCJhZG1pbiI6ZmFsc2UsImlhdCI6MTcxMTQ1NzY3N30.wPx7k58eJuSb2l_l3BYFOu41h-1jSgKJcfagEMC8NmQ

At the end of this lab, you will be able to:

* Describe the main components of a chatbot
* Explain what an LLM is
* Select an LLM for your application
* Describe how a transformer essentially works
* Feed input into a transformer (tokenization)
* Program your own simple chatbot in Python


***Note: Choosing the model.
https://huggingface.co/models


Intro: How does a chatbot work?
A chatbot is a computer program that takes a text input, and returns a corresponding text output.

Chatbots use a special kind of computer program called a transformer, which is like its brain. Inside this brain, there is something
called a language model (LLM), which helps the chatbot understand and generate human-like responses. It deciphers many examples of 
human conversations it has seen prior to responding in a sensible manner.

Transformers and LLMs work together within a chatbot to enable conversation. Here's a simplified explanation of how they interact:

* Input processing: When you send a message to the chatbot, the transformer helps process your input. 
It breaks down your message into smaller parts and represents them in a way that the chatbot can understand. Each part is called a token.

* Understanding context: The transformer passes these tokens to the LLM, which is a language model trained on lots of text data. 
The LLM has learned patterns and meanings from this data, so it tries to understand the context of your message based on what it has learned.

* Generating response: Once the LLM understands your message, it generates a response based on its understanding. 
The transformer then takes this response and converts it into a format that can be easily sent back to you.

* Iterative conversation: As the conversation continues, this process repeats. 
The transformer and LLM work together to process each new input message, understand the context, and generate a relevant response.

Resume: 
The key is that the LLM learns from a large amount of text data to understand language patterns and generate meaningful responses. 
The transformer helps with the technical aspects of processing and representing the input/output data, allowing the LLM to focus on understanding and generating language.


****Notes about Tokenization of user prompt and chat history
Tokens in NLP are individual units or elements that text or sentences are divided into. 
Tokenization or vectorization is the process of converting tokens into numerical representations.

***General code:

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# Chosen model
model_name = "facebook/blenderbot-400M-distill"

# Load model (download on first run and reference local installation for consequent runs)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

conversation_history = []

# Initializing history object
history_string = "\n".join(conversation_history)

# Example of text prompt
input_text ="hello, how are you doing?"

# Gathering both prompt text and history text and converting into token inputs to feed the model
inputs = tokenizer.encode_plus(history_string, input_text, return_tensors="pt")
print(inputs)

# Result:
# {'input_ids': tensor([[1710,   86,   19,  544,  366,  304,  929,   38]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}

# This attribute provides a mapping of pretrained models to their corresponding vocabulary files.
tokenizer.pretrained_vocab_files_map

# Model's results as a tokens:
outputs = model.generate(**inputs)
print(outputs)
# Results:
# tensor([[   1,  281,  476,  929,  731,   21,  281,  632,  929,  712,  731,   21, 855,  366,  304,   38,  946,  304,  360,  463, 5459, 7930,   38,    2]])

# This step parsing or converting the outpu token by the model to a texplain:
response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
print(response)

# Final output:
# I'm doing well. I am doing very well. How are you? Do you have any hobbies?


*****************
***Concise code from Lab wiht a LOOP on the line 13 / TO REPEAT the procesos automatically:

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# Chosen model
model_name = "facebook/blenderbot-400M-distill"

# Load model (download on first run and reference local installation for consequent runs)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

conversation_history = []

#(Ãºltimo paso/ last step): Repeat : put everything in a loop and run a whole conversation!
while True:

    # Initializing history object
    history_string = "\n".join(conversation_history)

    # Example of text prompt
    input_text ="hello, how are you doing?"

    # Gathering both prompt text and history text and converting into token inputs to feed the model
    inputs = tokenizer.encode_plus(history_string, input_text, return_tensors="pt")
    print(inputs)

    # Result:
    # {'input_ids': tensor([[1710,   86,   19,  544,  366,  304,  929,   38]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}

    # This attribute provides a mapping of pretrained models to their corresponding vocabulary files.
    tokenizer.pretrained_vocab_files_map

    # Model's results as a tokens:
    outputs = model.generate(**inputs)
    print(outputs)
    # Results:
    # tensor([[   1,  281,  476,  929,  731,   21,  281,  632,  929,  712,  731,   21, 855,  366,  304,   38,  946,  304,  360,  463, 5459, 7930,   38,    2]])

    # This step parsing or converting the outpu token by the model to a texplain:
    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
    print(response)

    # Final output:
    # I'm doing well. I am doing very well. How are you? Do you have any hobbies?

    # ********************************
    # Update conversation history
    # Add interaction to conversation history

    conversation_history.append(input_text)
    conversation_history.append(response)
    print(conversation_history)



******************************************************************************















